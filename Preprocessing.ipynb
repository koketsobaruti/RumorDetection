{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43323b1e-e883-42c2-b1db-4181d93c1cae",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ded1df86-8199-438e-ae4e-371974264233",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string, unicodedata\n",
    "import nltk\n",
    "import contractions\n",
    "import inflect\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "be2b097a-45df-4c97-b934-789a3c102868",
   "metadata": {},
   "outputs": [],
   "source": [
    "from num2words import num2words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fbb641b4-6eaa-4a7c-848c-24e03866224c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting num2words\n",
      "  Downloading num2words-0.5.13-py3-none-any.whl (143 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.3/143.3 KB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hCollecting docopt>=0.6.2\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: docopt\n",
      "  Building wheel for docopt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13723 sha256=c7e899214ddac1d40f53e9fdddf952906fdb104c2ec6c61ec87f3fc9d7a02364\n",
      "  Stored in directory: /home/sct/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
      "Successfully built docopt\n",
      "Installing collected packages: docopt, num2words\n",
      "Successfully installed docopt-0.6.2 num2words-0.5.13\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install num2words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "15ef0135-a94a-4f63-bdf0-d5060c1405b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting inflect\n",
      "  Downloading inflect-7.2.1-py3-none-any.whl (34 kB)\n",
      "Collecting typeguard>=4.0.1\n",
      "  Downloading typeguard-4.2.1-py3-none-any.whl (34 kB)\n",
      "Collecting more-itertools\n",
      "  Downloading more_itertools-10.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 KB\u001b[0m \u001b[31m219.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /home/sct/my_project_dir/my_project_env/lib/python3.10/site-packages (from inflect) (4.11.0)\n",
      "Installing collected packages: typeguard, more-itertools, inflect\n",
      "Successfully installed inflect-7.2.1 more-itertools-10.2.0 typeguard-4.2.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install inflect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fa1d43d0-d5cd-4d0e-8144-bb188f02bec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting contractions\n",
      "  Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
      "Collecting textsearch>=0.0.21\n",
      "  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
      "Collecting anyascii\n",
      "  Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 KB\u001b[0m \u001b[31m72.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hCollecting pyahocorasick\n",
      "  Downloading pyahocorasick-2.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.7/110.7 KB\u001b[0m \u001b[31m227.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
      "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.1.0 textsearch-0.0.24\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5b0a9df-c8e3-45a4-b44e-e9e6a5431caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0m\n",
      "\u001b[?25hRequirement already satisfied: joblib in /home/sct/my_project_dir/my_project_env/lib/python3.10/site-packages (from nltk) (1.4.0)\n",
      "Collecting click\n",
      "  Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.9/97.9 KB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m9 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: regex>=2021.8.3 in /home/sct/my_project_dir/my_project_env/lib/python3.10/site-packages (from nltk) (2024.4.28)\n",
      "Requirement already satisfied: tqdm in /home/sct/my_project_dir/my_project_env/lib/python3.10/site-packages (from nltk) (4.66.2)\n",
      "Installing collected packages: click, nltk\n",
      "Successfully installed click-8.1.7 nltk-3.8.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff7e4af5-6663-4c6a-b91a-cfffa42efe3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to /home/sct/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
      "[nltk_data]    | Downloading package alpino to /home/sct/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /home/sct/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     /home/sct/nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     /home/sct/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
      "[nltk_data]    | Downloading package bcp47 to /home/sct/nltk_data...\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     /home/sct/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     /home/sct/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     /home/sct/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
      "[nltk_data]    | Downloading package brown to /home/sct/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     /home/sct/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     /home/sct/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     /home/sct/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
      "[nltk_data]    | Downloading package chat80 to /home/sct/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /home/sct/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
      "[nltk_data]    | Downloading package cmudict to /home/sct/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     /home/sct/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     /home/sct/nltk_data...\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     /home/sct/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     /home/sct/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     /home/sct/nltk_data...\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     /home/sct/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /home/sct/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
      "[nltk_data]    | Downloading package dolch to /home/sct/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     /home/sct/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
      "[nltk_data]    | Downloading package extended_omw to\n",
      "[nltk_data]    |     /home/sct/nltk_data...\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     /home/sct/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     /home/sct/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     /home/sct/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /home/sct/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
      "[nltk_data]    | Downloading package genesis to /home/sct/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /home/sct/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
      "[nltk_data]    | Downloading package ieer to /home/sct/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /home/sct/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
      "[nltk_data]    | Downloading package indian to /home/sct/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
      "[nltk_data]    | Downloading package jeita to /home/sct/nltk_data...\n",
      "[nltk_data]    | Downloading package kimmo to /home/sct/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
      "[nltk_data]    | Downloading package knbc to /home/sct/nltk_data...\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     /home/sct/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     /home/sct/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     /home/sct/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
      "[nltk_data]    | Downloading package machado to /home/sct/nltk_data...\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     /home/sct/nltk_data...\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /home/sct/nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /home/sct/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     /home/sct/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /home/sct/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     /home/sct/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     /home/sct/nltk_data...\n",
      "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
      "[nltk_data]    | Downloading package names to /home/sct/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/names.zip.\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     /home/sct/nltk_data...\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     /home/sct/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     /home/sct/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
      "[nltk_data]    | Downloading package omw to /home/sct/nltk_data...\n",
      "[nltk_data]    | Downloading package omw-1.4 to /home/sct/nltk_data...\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     /home/sct/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /home/sct/nltk_data...\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     /home/sct/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
      "[nltk_data]    | Downloading package pe08 to /home/sct/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pe08.zip.\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     /home/sct/nltk_data...\n",
      "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
      "[nltk_data]    | Downloading package pil to /home/sct/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
      "[nltk_data]    | Downloading package pl196x to /home/sct/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     /home/sct/nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     /home/sct/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     /home/sct/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     /home/sct/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     /home/sct/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     /home/sct/nltk_data...\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     /home/sct/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
      "[nltk_data]    | Downloading package ptb to /home/sct/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
      "[nltk_data]    | Downloading package punkt to /home/sct/nltk_data...\n",
      "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data]    | Downloading package qc to /home/sct/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
      "[nltk_data]    | Downloading package reuters to /home/sct/nltk_data...\n",
      "[nltk_data]    | Downloading package rslp to /home/sct/nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
      "[nltk_data]    | Downloading package rte to /home/sct/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     /home/sct/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
      "[nltk_data]    | Downloading package semcor to /home/sct/nltk_data...\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     /home/sct/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     /home/sct/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     /home/sct/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /home/sct/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     /home/sct/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     /home/sct/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /home/sct/nltk_data...\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     /home/sct/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     /home/sct/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /home/sct/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     /home/sct/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
      "[nltk_data]    | Downloading package swadesh to /home/sct/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     /home/sct/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
      "[nltk_data]    | Downloading package tagsets to /home/sct/nltk_data...\n",
      "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
      "[nltk_data]    | Downloading package timit to /home/sct/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
      "[nltk_data]    | Downloading package toolbox to /home/sct/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /home/sct/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /home/sct/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
      "[nltk_data]    | Downloading package udhr to /home/sct/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
      "[nltk_data]    | Downloading package udhr2 to /home/sct/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /home/sct/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     /home/sct/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     /home/sct/nltk_data...\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     /home/sct/nltk_data...\n",
      "[nltk_data]    | Downloading package verbnet to /home/sct/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     /home/sct/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
      "[nltk_data]    | Downloading package webtext to /home/sct/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     /home/sct/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     /home/sct/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
      "[nltk_data]    | Downloading package wordnet to /home/sct/nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     /home/sct/nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet2022 to\n",
      "[nltk_data]    |     /home/sct/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet2022.zip.\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     /home/sct/nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /home/sct/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to /home/sct/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/words.zip.\n",
      "[nltk_data]    | Downloading package ycoe to /home/sct/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f24c04-63cd-423f-8ff3-b2e91b1826c0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2501920-0216-480b-9625-9645ff139d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 420 entries, 0 to 419\n",
      "Data columns (total 7 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   user_id         420 non-null    int64 \n",
      " 1   tweet_id        420 non-null    int64 \n",
      " 2   tweet_text      420 non-null    object\n",
      " 3   favorite_count  420 non-null    int64 \n",
      " 4   retweet_count   420 non-null    int64 \n",
      " 5   created_at      420 non-null    object\n",
      " 6   label           420 non-null    object\n",
      "dtypes: int64(4), object(3)\n",
      "memory usage: 23.1+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 470 entries, 0 to 469\n",
      "Data columns (total 7 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   user_id         470 non-null    int64 \n",
      " 1   tweet_id        470 non-null    int64 \n",
      " 2   tweet_text      470 non-null    object\n",
      " 3   favorite_count  470 non-null    int64 \n",
      " 4   retweet_count   470 non-null    int64 \n",
      " 5   created_at      470 non-null    object\n",
      " 6   label           470 non-null    object\n",
      "dtypes: int64(4), object(3)\n",
      "memory usage: 25.8+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "nr_source_df = pd.read_csv('non_rumor_source_tweet_data.csv')\n",
    "nr_source_df.info()\n",
    "\n",
    "r_source_df = pd.read_csv('rumor_source_tweet_data.csv')\n",
    "r_source_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f81dd412-9d48-4dd7-9d52-4ca521a3d69f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ottawa police are confirming a shooting at the...</td>\n",
       "      <td>non-rumor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Passersby working on shooting victim. From the...</td>\n",
       "      <td>non-rumor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Shooting at the war memorial in Ottawa. http:/...</td>\n",
       "      <td>non-rumor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>After today we will officially be done shootin...</td>\n",
       "      <td>non-rumor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KEY: Nothing in Wilson's story in new autopsy ...</td>\n",
       "      <td>non-rumor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text      label\n",
       "0  Ottawa police are confirming a shooting at the...  non-rumor\n",
       "1  Passersby working on shooting victim. From the...  non-rumor\n",
       "2  Shooting at the war memorial in Ottawa. http:/...  non-rumor\n",
       "3  After today we will officially be done shootin...  non-rumor\n",
       "4  KEY: Nothing in Wilson's story in new autopsy ...  non-rumor"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = nr_source_df['tweet_text']\n",
    "label = nr_source_df['label']\n",
    "data = {'text':text, 'label':label}\n",
    "nr_data = pd.DataFrame(data)\n",
    "nr_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fef8ec4-6383-4d05-8a48-a19418ee1582",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Recap: Gunman shot dead inside Parliament buil...</td>\n",
       "      <td>rumor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Soldier killed at war memorial identified as N...</td>\n",
       "      <td>rumor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Soldier killed at war memorial identified as C...</td>\n",
       "      <td>rumor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BREAKING: Two or three gunmen were involved in...</td>\n",
       "      <td>rumor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>All 3 patients injured in #OttawaShooting rele...</td>\n",
       "      <td>rumor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  Recap: Gunman shot dead inside Parliament buil...  rumor\n",
       "1  Soldier killed at war memorial identified as N...  rumor\n",
       "2  Soldier killed at war memorial identified as C...  rumor\n",
       "3  BREAKING: Two or three gunmen were involved in...  rumor\n",
       "4  All 3 patients injured in #OttawaShooting rele...  rumor"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = r_source_df['tweet_text']\n",
    "label = r_source_df['label']\n",
    "data = {'text':text, 'label':label}\n",
    "r_data = pd.DataFrame(data)\n",
    "r_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b92fbbf-9e5b-46e5-8b70-7aff6c18ab4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 890 entries, 0 to 469\n",
      "Data columns (total 7 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   user_id         890 non-null    int64 \n",
      " 1   tweet_id        890 non-null    int64 \n",
      " 2   tweet_text      890 non-null    object\n",
      " 3   favorite_count  890 non-null    int64 \n",
      " 4   retweet_count   890 non-null    int64 \n",
      " 5   created_at      890 non-null    object\n",
      " 6   label           890 non-null    object\n",
      "dtypes: int64(4), object(3)\n",
      "memory usage: 55.6+ KB\n"
     ]
    }
   ],
   "source": [
    "#combine datasets of rumors and non-rumors\n",
    "frames = [nr_source_df,r_source_df]\n",
    "df = pd.concat(frames)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "35c1b72d-d804-40bc-a2a2-76b94fe15671",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ottawa police are confirming a shooting at the...</td>\n",
       "      <td>non-rumor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Passersby working on shooting victim. From the...</td>\n",
       "      <td>non-rumor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Shooting at the war memorial in Ottawa. http:/...</td>\n",
       "      <td>non-rumor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>After today we will officially be done shootin...</td>\n",
       "      <td>non-rumor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KEY: Nothing in Wilson's story in new autopsy ...</td>\n",
       "      <td>non-rumor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text      label\n",
       "0  Ottawa police are confirming a shooting at the...  non-rumor\n",
       "1  Passersby working on shooting victim. From the...  non-rumor\n",
       "2  Shooting at the war memorial in Ottawa. http:/...  non-rumor\n",
       "3  After today we will officially be done shootin...  non-rumor\n",
       "4  KEY: Nothing in Wilson's story in new autopsy ...  non-rumor"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#select the text and labels for conducting preprocessing\n",
    "text = df['tweet_text']\n",
    "label = df['label']\n",
    "data = {'text':text, 'label':label}\n",
    "df = pd.DataFrame(data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "eef1c6b5-10c3-4929-b3dc-7e762d276296",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@ctvottawa confirms there were 3 separate sho...</td>\n",
       "      <td>rumor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Scene at Ottawa war memorial. Soldier on duty ...</td>\n",
       "      <td>rumor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dramatic photos from @wcuddington on the scene...</td>\n",
       "      <td>non-rumor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Canadian cabinet minister Jason Kenney says \"C...</td>\n",
       "      <td>non-rumor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Our phone lines are open 24/7 for your use, if...</td>\n",
       "      <td>non-rumor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MORE: Canadian soldier shot at war memorial ha...</td>\n",
       "      <td>rumor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MORE: A spokesman says Prime Minister Stephen ...</td>\n",
       "      <td>non-rumor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Security bulletin from Parliament Hill: #ottaw...</td>\n",
       "      <td>non-rumor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sad about #OttawaShooting. Worse, shooter migh...</td>\n",
       "      <td>non-rumor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>#RCMP news conference on #Ottawa shootings exp...</td>\n",
       "      <td>rumor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text      label\n",
       "0  .@ctvottawa confirms there were 3 separate sho...      rumor\n",
       "1  Scene at Ottawa war memorial. Soldier on duty ...      rumor\n",
       "2  Dramatic photos from @wcuddington on the scene...  non-rumor\n",
       "3  Canadian cabinet minister Jason Kenney says \"C...  non-rumor\n",
       "4  Our phone lines are open 24/7 for your use, if...  non-rumor\n",
       "5  MORE: Canadian soldier shot at war memorial ha...      rumor\n",
       "6  MORE: A spokesman says Prime Minister Stephen ...  non-rumor\n",
       "7  Security bulletin from Parliament Hill: #ottaw...  non-rumor\n",
       "8  Sad about #OttawaShooting. Worse, shooter migh...  non-rumor\n",
       "9  #RCMP news conference on #Ottawa shootings exp...      rumor"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shuffle the DataFrame rows\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "df = shuffle(df).reset_index(drop=True)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc45350-4556-4b5a-a95f-44c768bceec4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Upper to Lowercase Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b087d8f-3665-4f81-a0d9-0ca7520c9c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lowercase(df):\n",
    "    df['text'] = df['text'].apply(lambda x: x.lower() if isinstance(x, str) else x)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298a527d-2734-48bf-a2fe-60c6045b9b88",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Stopword removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20df8457-1592-49a3-9991-ee968b9ff850",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to remove stopwords\n",
    "def remove_stopwords(df):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    df['text'] = df['text'].apply(lambda x: ' '.join([word for word in x.split() if word.lower() not in stop_words]))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58dcc97-80aa-4cb3-9309-b57eb11c70a3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Remove URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2de649b9-4bbf-432a-8198-7e6162d8a41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_URL(df):\n",
    "    \"\"\"Remove URLs from a sample string\"\"\"\n",
    "    df['text'] = df['text'].apply(lambda x:re.sub(r\"http\\S+\", \"\", x))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62c50e36-ee23-4c06-8d85-6b6d0c7909e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = remove_URL(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc8e66d-cc41-4498-afd9-82c4bac670d3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Remove Non-ASCII words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa05e666-a102-40a9-a8ec-a116f606a7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_ascii(df):\n",
    "    df['text'] = df['text'].apply(lambda x:' '.join([unicodedata.normalize('NFKD', x).encode('ascii', 'ignore')\n",
    "                                                     .decode('utf-8', 'ignore') for words in x.split()]))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6baf6d51-9dce-45bc-b498-8d21c513736e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Remove contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8f423e1-0a83-4bfa-9def-fac3c5e9c574",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_contractions(df):\n",
    "    \"\"\"Replace contractions in string of text\"\"\"\n",
    "    df['text'] = df['text'].apply(lambda x:contractions.fix(x))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebb895e-f890-4f7b-8c01-a92da5b2dd78",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Remove puntuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65d2b6c3-5058-4d29-99d7-b28840d61ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(df):\n",
    "    pattern = r'\\'\\S+'\n",
    "    df['text'] = df['text'] .apply(lambda x: re.sub(pattern,\"\", str(x)))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ee4bb3-d746-470a-9949-5cf07e76bd3d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Replace numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c2b663d-f263-47a1-9e9c-59748563ed55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_numbers(df):\n",
    "    p = inflect.engine()\n",
    "    df['text'] = df['text'].apply(lambda text: ' '.join(num2words(int(word)) if word.isdigit() else word for word in text.split()))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8f78fa-8bed-48f9-826a-ff7c89b1d8f7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b267cce-21d2-45ae-9117-fd22c15deb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_sentence(input_string):\n",
    "    # Perform lemmatization\n",
    "    words = word_tokenize(input_string)\n",
    "    # print(\"{0:20}{1:20}\".format(\"--Word--\", \"--Lemma--\"))\n",
    "    lemmatized_sentence = reduce(lambda x, y: x + \" \" + wnl.lemmatize(y, pos=\"v\"), words, \"\")\n",
    "    return lemmatized_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "92f18fcc-ef01-437f-868a-c34a24d36a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(df):\n",
    "    try:\n",
    "        for i in df.index:\n",
    "            in_string = df.loc[i,'text']\n",
    "            df.loc[i,'text'] = lemmatize_sentence(in_string)\n",
    "    except Exception as inst:\n",
    "        print(type(inst))  # the exception type\n",
    "        print(inst.args)  # arguments stored in .args\n",
    "        print(inst)         # __str__ allows args to be printed directly,\n",
    "                        # but may be overridden in exception subclasses\n",
    "        #print(count)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568cbd5a-eec2-4ce2-9b05-86f7435aed8c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "023601d8-a2f7-4bed-85fe-767d8b5fe78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(df):\n",
    "    df['text'] = df['text'].apply(lambda text: ' '.join(filter(lambda new_word: new_word != '', \n",
    "                                                               [re.sub(r'[^\\w\\s]', '', word) for word in text.split()])))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e736665c-b947-42d9-b533-450d50c74742",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efc472f1-6298-43aa-afdc-0af76af22a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(df_in):\n",
    "    df = remove_non_ascii(df_in)\n",
    "    df = to_lowercase(df)\n",
    "    df = remove_punctuation(df)\n",
    "    df = replace_numbers(df)\n",
    "    #df = remove_stopwords(df)\n",
    "    #df = lemmatize(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b111a2fe-415d-4ad3-8412-35d7fd60742a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    updated_df = remove_URL(df)\n",
    "    updated_df = replace_contractions(updated_df)\n",
    "    # Tokenize\n",
    "    #words = nltk.word_tokenize(sample)\n",
    "    normalized_df = normalize(updated_df)\n",
    "    # Normalize\n",
    "    return normalized_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2b31036b-a25d-4904-9c4c-ae89b6b5dac7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ottawa police are confirming a shooting at the...</td>\n",
       "      <td>non-rumor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Passersby working on shooting victim. From the...</td>\n",
       "      <td>non-rumor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Shooting at the war memorial in Ottawa. http:/...</td>\n",
       "      <td>non-rumor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>After today we will officially be done shootin...</td>\n",
       "      <td>non-rumor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KEY: Nothing in Wilson's story in new autopsy ...</td>\n",
       "      <td>non-rumor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Ottawa police confirm a shooting at Canada's N...</td>\n",
       "      <td>non-rumor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>#BREAKING: Shooting reported at the War Memori...</td>\n",
       "      <td>non-rumor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>#BREAKINGNEWS: Update - One person shot at War...</td>\n",
       "      <td>non-rumor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The scene at the War Memorial in Ottawa http:/...</td>\n",
       "      <td>non-rumor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Ottawa police tactical officers are here and v...</td>\n",
       "      <td>non-rumor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text      label\n",
       "0  Ottawa police are confirming a shooting at the...  non-rumor\n",
       "1  Passersby working on shooting victim. From the...  non-rumor\n",
       "2  Shooting at the war memorial in Ottawa. http:/...  non-rumor\n",
       "3  After today we will officially be done shootin...  non-rumor\n",
       "4  KEY: Nothing in Wilson's story in new autopsy ...  non-rumor\n",
       "5  Ottawa police confirm a shooting at Canada's N...  non-rumor\n",
       "6  #BREAKING: Shooting reported at the War Memori...  non-rumor\n",
       "7  #BREAKINGNEWS: Update - One person shot at War...  non-rumor\n",
       "8  The scene at the War Memorial in Ottawa http:/...  non-rumor\n",
       "9  Ottawa police tactical officers are here and v...  non-rumor"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ca0e01eb-76e6-40e5-a9ba-6571abc93d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label\n",
      "0  recap gunman shot dead inside parliament build...  rumor\n",
      "1  soldier killed at war memorial identified as n...  rumor\n",
      "2  soldier killed at war memorial identified as c...  rumor\n",
      "3  breaking two or three gunmen were involved in ...  rumor\n",
      "4  all three patients injured in ottawashooting r...  rumor\n"
     ]
    }
   ],
   "source": [
    "rumor_data = preprocess(r_data)\n",
    "print(rumor_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d0a6a74e-63ea-4efa-b2e8-85a55758644a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rumor_data.to_csv('preprocessed_rumor_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "509fd9a7-9609-4e50-a894-4e5bc0d46e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text      label\n",
      "0  ottawa police are confirming a shooting at the...  non-rumor\n",
      "1  passersby working on shooting victim from the ...  non-rumor\n",
      "2  shooting at the war memorial in ottawa shootin...  non-rumor\n",
      "3  after today we will officially be done shootin...  non-rumor\n",
      "4  key nothing in wilsons story in new autopsy sa...  non-rumor\n"
     ]
    }
   ],
   "source": [
    "non_rumor_data = preprocess(nr_data)\n",
    "print(non_rumor_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7a35b85f-852a-4663-aaa8-39dbd103846e",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_rumor_data.to_csv('preprocessed_non_rumor_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2801c94b-e5ef-4ce2-b1a0-5fb43f575c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text      label\n",
      "0  ottawa police are confirming a shooting at the...  non-rumor\n",
      "1  passersby working on shooting victim from the ...  non-rumor\n",
      "2  shooting at the war memorial in ottawa shootin...  non-rumor\n",
      "3  after today we will officially be done shootin...  non-rumor\n",
      "4  key nothing in wilsons story in new autopsy sa...  non-rumor\n"
     ]
    }
   ],
   "source": [
    "preprocessed_df = preprocess(df)\n",
    "print(preprocessed_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "74a9944a-64ff-44fe-b27a-08bfc9d238e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ottawa police are confirming a shooting at the...</td>\n",
       "      <td>non-rumor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>passersby working on shooting victim from the ...</td>\n",
       "      <td>non-rumor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>shooting at the war memorial in ottawa shootin...</td>\n",
       "      <td>non-rumor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>after today we will officially be done shootin...</td>\n",
       "      <td>non-rumor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>key nothing in wilsons story in new autopsy sa...</td>\n",
       "      <td>non-rumor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text      label\n",
       "0  ottawa police are confirming a shooting at the...  non-rumor\n",
       "1  passersby working on shooting victim from the ...  non-rumor\n",
       "2  shooting at the war memorial in ottawa shootin...  non-rumor\n",
       "3  after today we will officially be done shootin...  non-rumor\n",
       "4  key nothing in wilsons story in new autopsy sa...  non-rumor"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "97e00cc5-8e58-4321-9e5f-c060c7536395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 890 entries, 0 to 889\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    890 non-null    object\n",
      " 1   label   890 non-null    object\n",
      "dtypes: object(2)\n",
      "memory usage: 14.0+ KB\n"
     ]
    }
   ],
   "source": [
    "preprocessed_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "09575464-87fc-412b-ba66-52e240e450f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_df.to_csv('preprocessed_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c5ae26-e047-4b86-8f2c-39668b1bb7df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

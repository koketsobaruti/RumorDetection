{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde8e935-7b07-4f58-b780-57fc790d2f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from keras.layers import Input, Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout, Concatenate, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e669286-cd82-418f-b94f-6ad2548adf44",
   "metadata": {},
   "source": [
    "## CNN that takes in text and numerical features for rumor detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13888ad-0463-46e4-83ee-5f171aaac983",
   "metadata": {},
   "source": [
    "### Retrieve Test and Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b83e8d4-78fb-4ba7-b1b5-742abc86a09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('updated_data.csv')\n",
    "label = data['label']\n",
    "text_data = data['tweet_text']\n",
    "numerical_data = data[['retweet_count', 'favorite_count', 'follower_count', 'difference', 'ratio', 'verified', 'time_delayed (min)'\n",
    "                 , 'neg_sent', 'pos_sent', 'comp_sent']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e1f48c-5036-483b-b936-a23ad2fd995a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train_text, X_train_num, X_test_text, X_test_num, y_train, y_test = train_test_split([text_data, numerical_data], label, test_size=0.2, random_state=42, stratify = label)\n",
    "\n",
    "print(f'Shape of X_train_text: {X_train_text.shape}'), print(f'Shape of X_train_num: {X_train_num.shape}')\n",
    "print(f'Shape of X_test_text: {X_test_text.shape}'), print(f'Shape of X_test_num: {X_test_num.shape}')\n",
    "print(f'Shape of y_train: {y_train.shape}'), print(f'Shape of y_test: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6274f6ae-88e2-44e6-ad55-a0b1c93a5390",
   "metadata": {},
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f12750-4321-4b43-b961-fa42622ba8e2",
   "metadata": {},
   "source": [
    "#### Preprocess Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffac87fa-1832-4acc-9420-b6dd7add6a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "elmo = hub.load(\"https://tfhub.dev/google/elmo/3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453088f0-9f3b-4cc9-a3de-7999d1bcb8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def elmo_embeddings(input_tensor):\n",
    "    text_list = tf.squeeze(input_tensor, axis=1)\n",
    "    embeddings = elmo(text_list)[\"elmo\"]\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efc9b06-9361-4edf-92e6-b4fc36696a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_in_batches(texts, batch_size=32):\n",
    "    elmo_embed_np = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "        batch_embeddings = elmo_embeddings(batch_texts)\n",
    "        elmo_embed_np.extend([embed.numpy() for embed in batch_embeddings])\n",
    "    return np.array(elmo_embed_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d273a4b5-5810-4877-8b4f-836633b5f073",
   "metadata": {},
   "outputs": [],
   "source": [
    "elmo_train = process_in_batches(X_train_text)\n",
    "elmo_test = process_in_batches(X_test_text)\n",
    "\n",
    "print(f'SHape of X_test_text: {X_test_text.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c98d24-e039-4cd7-8f9a-a4f2bbe91814",
   "metadata": {},
   "source": [
    "#### Preprocess Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b605592-7c10-4b99-aa69-b696a16cc1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "norm_train_num=scaler.transform_fit(X_train_num)\n",
    "norm_test_num = scaler.transform_fit(X_test_num)\n",
    "\n",
    "print(norm_test_num.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73c61a7-c3fb-4e20-b494-6f165fc6dd48",
   "metadata": {},
   "source": [
    "#### Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a777e52-87f4-4b41-ae48-3862e7c5b7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model():\n",
    "    # Define textual input\n",
    "    input_layer = Input(shape=(1,), dtype=tf.string, name='input_text')\n",
    "    # Obtain ELMo embeddings\n",
    "    embedding_layer = elmo_embeddings(input_layer)\n",
    "    conv_layer = Conv1D(filters=128, kernel_size=5, activation='relu')(embedding_layer)\n",
    "    pooling_layer = GlobalMaxPooling1D()(conv_layer)\n",
    "    flattened_text = Flatten()(pooling_layer)\n",
    "    \n",
    "    # Define numerical input\n",
    "    input_numeric = Input(shape=(10,), name=\"input_numeric\")\n",
    "    dense_num = Dense(128, activation='relu')(input_num)\n",
    "    dropout_num = Dropout(0.5)(dense_num)\n",
    "    \n",
    "    # Concatenate textual and numerical inputs\n",
    "    concatenated_layer = Concatenate()([flattened_text, dropout_num])\n",
    "    dense_merged = Dense(128, activation='relu')(concatenated_layer)\n",
    "    dropout_merged = Dropout(0.5)(dense_merged)\n",
    "    output = Dense(1, activation='sigmoid')(dropout_merged)\n",
    "    \n",
    "    \n",
    "    model = Model(inputs=[input_layer, input_numeric], outputs=output)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=Adam(learning_rate = 0.001),\n",
    "                  metrics=['accuracy', 'recall', 'Precision', 'FalseNegatives'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546dc840-90be-4cff-8970-0cd4bec2aa31",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cnn_model()\n",
    "history = model.fit( [elmo_train, norm_train_num], \n",
    "                    y_train, \n",
    "                    batch_size=16, \n",
    "                    epochs=1, \n",
    "                    validation_data=0.2,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc69eca2-c56d-4cbc-9440-8911aeca0ca8",
   "metadata": {},
   "source": [
    "#### Code for working on Bayesian Optimization (work in progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93bc9b2-1c03-4cb3-9eee-9ac278e39b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_acc = make_scorer(accuracy_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ae7156-8998-474e-83a1-ec413631a20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_cnn():\n",
    "    ## build model architecture\n",
    "    input_x = Input(shape=(1034,))\n",
    "    print(f\"Concatenated model: {x.shape}\")\n",
    "    ## Reshape to fit Conv1D input\n",
    "    x = Reshape((x.shape[1], 1))(x)\n",
    "    print(f\"Concatenated model after reshape: {x.shape}\")\n",
    "    ## Define Convolutional Layers\n",
    "    for i in range(conv_layers):\n",
    "        ### create a 1D-conv layers with given number of filters and kernel sizes and a given activation function \n",
    "        ### from the function list with input staring from the concatenated neurons\n",
    "        x = Conv1D(filters=conv_filters, kernel_size=conv_kernel_size, activation=activation)(x)\n",
    "        print(f\"Concatenated model after iteration {i+1}: {x.shape}\")\n",
    "        if normalization > 0.5:\n",
    "            x = BatchNormalization()(x)\n",
    "    ## apply pooling to the given output x\n",
    "    x = GlobalMaxPooling1D()(x)\n",
    "    print(f\"Shape after GlobalMaxPooling1D layer {i + 1}: {x.shape}\")\n",
    "    ## flatten the layer \n",
    "    x = Flatten()(x)\n",
    "\n",
    "    ## Define dense layers\n",
    "    for i in range(dense_layers):\n",
    "        ### define dense layer with the given amout of neurons and activation function\n",
    "        x = Dense(neurons, activation=activation)(x)\n",
    "\n",
    "        if dropout>0.5:\n",
    "            x = Dropout(dropout_rate, seed=123)()\n",
    "\n",
    "    ## Define an output layer where the final prediction is made\n",
    "    output = Dense(1, activation='sigmoid')(x)\n",
    "    ## Define the model\n",
    "    model = Model(inputs=input_x, outputs = output)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy', 'recall', 'Precision', 'FalseNegatives'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49c03ed-1675-44d8-b2ad-5b992a37a008",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_num_cnn(neurons, learning_rate, batch_size, epochs,\n",
    "                conv_layers, conv_filters, conv_kernel_size,\n",
    "                dense_layers, normalization, dropout, dropout_rate):\n",
    "\n",
    "    #set variables by getting the next int and set parameters for optimizer and activation function\n",
    "    neurons = round(neurons)\n",
    "    activation ='relu'\n",
    "    optimizer = Adam(learning_rate = 0.001)\n",
    "    batch_size = round(batch_size)\n",
    "    learning_rate = round(learning_rate)\n",
    "    epochs = round(epochs)\n",
    "    conv_layers = round(conv_layers)\n",
    "    conv_filters = round(conv_filters)\n",
    "    conv_kernel_size = round(conv_kernel_size)\n",
    "    dense_layers = round(dense_layers)\n",
    "\n",
    "    # Define the Keras model with the provided parameters\n",
    "    def create_model():\n",
    "        return cnn_model(\n",
    "            neurons=neurons,\n",
    "            activation=activation,\n",
    "            optimizer=optimizer,\n",
    "            learning_rate=learning_rate,\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            conv_layers=conv_layers,\n",
    "            conv_filters=conv_filters,\n",
    "            conv_kernel_size=conv_kernel_size,\n",
    "            dense_layers=dense_layers,\n",
    "            normalization=normalization,\n",
    "            dropout=dropout,\n",
    "            dropout_rate=dropout_rate\n",
    "        )\n",
    "\n",
    "    ## Add early stopping to prevent model from continuing to find optimal parameters after the 20th\n",
    "    es = EarlyStopping(monitor='accuracy', mode = 'max', verbose=0, patience=20)\n",
    "    nn = KerasClassifier(build_fn =model, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "    kfold = StratifiedKFold(n_splits =5, shuffle=True, random_state=123)\n",
    "    score = cross_val_score(nn, X_train_norm, y_train, scoring=score_acc, cv=kfold, fit_params={'callbacks':[es]}).mean()\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb32835-b35e-4b52-8045-069fd6b8d24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "params_cnn = {\n",
    "    'neurons': (10, 100),\n",
    "    'learning_rate': (0.001, 0.1),\n",
    "    'batch_size': (16, 128),\n",
    "    'epochs': (10, 50),\n",
    "    'conv_layers': (1, 3),\n",
    "    'conv_filters': (32, 128),\n",
    "    'conv_kernel_size': (2, 5),\n",
    "    'dense_layers': (1, 3),\n",
    "    'dropout': (0.0, 0.5),\n",
    "    'normalization': (0.0, 1.0),\n",
    "    'dropout_rate': (0.1, 0.5)\n",
    "}\n",
    "\n",
    "# Bayesian Optimization\n",
    "cnn_bo_optimizer = BayesianOptimization(text_num_cnn,pbounds=params_cnn,random_state=111)\n",
    "\n",
    "cnn_bo_optimizer.maximize(init_points=10, n_iter=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
